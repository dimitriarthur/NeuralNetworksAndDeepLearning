# Weight initialization

When we create our neural networks, we have to make choices for the initial weights and biases. Up to now, we've been choosing them according to a prescription which I discussed only briefly back in (Chapter 1)[link to chapter 1]. Just to remind you, that prescription was to choose both the weights and biases using independent Gaussian random variables, normalized to have mean 0 and standard deviation 1. While this approach has worked well, it was quite ad hoc, and it's worth revisiting to see if we can find a better way of setting our initial weights and biases, and perhaps help our neural networks learn faster.  

It turns out that we can do quite a bit better than initializing with normalized Gaussians. To see why, suppose we're working with a network with a large number - say 1,000 - of input neurons. And let's suppose we've used normalized Gaussians to initialize the weights connecting to the first hidden layer. For now I'm going to concentrate specifically on the weights connecting the input neurons to the first neuron in the hidden layer, and ignore the rest of the network:


<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/157573745-4af91c97-147b-46d2-9eae-828e946b0ce1.png" width="200"/><br>
  <b><i><a name="2.2"> Figure 1.0:</a></b> Weights of neurons in the hidden layer.</i>
</p>

We'll suppose for simplicity that we're trying to train using a training input x in which half the input neurons are on, i.e., set to 1, and half the input neurons are off, i.e., set to 0. The argument which follows applies more generally, but you'll get the gist from this special case. Let's consider the weighted sum z=\sum<sub>j</sub>w<sub>j</sub>x<sub>j</sub>+b of inputs to our hidden neuron. 500 terms in this sum vanish, because the corresponding input x<sub>j</sub> is zero. And so z is a sum over a total of 501 normalized Gaussian random variables, accounting for the 500 weight terms and the 1 extra bias term. Thus z is itself distributed as a Gaussian with mean zero and standard deviation 501<sup>1/2</sup>≈22.4. That is, z has a very broad Gaussian distribution, not sharply peaked at all:

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/157575463-8566c5e3-6ebc-4df3-b505-9467b543cf99.png" width="400"/><br>
  <b><i><a name="2.2"> Figure 1.1:</a></b> Z distribution.</i>
</p>

In particular, we can see from this graph that it's quite likely that |z| will be pretty large, i.e., either z≫1 or z≪−1. If that's the case then the output σ(z) from the hidden neuron will be very close to either 1 or 0. That means our hidden neuron will have saturated. And when that happens, as we know, making small changes in the weights will make only absolutely miniscule changes in the activation of our hidden neuron. That miniscule change in the activation of the hidden neuron will, in turn, barely affect the rest of the neurons in the network at all, and we'll see a correspondingly miniscule change in the cost function. As a result, those weights will only learn very slowly when we use the gradient descent algorithm. It's similar to the problem we discussed earlier in this chapter, in which output neurons which saturated on the wrong value caused learning to slow down. We addressed that earlier problem with a clever choice of cost function. Unfortunately, while that helped with saturated output neurons, it does nothing at all for the problem with saturated hidden neurons.

I've been talking about the weights input to the first hidden layer. Of course, similar arguments apply also to later hidden layers: if the weights in later hidden layers are initialized using normalized Gaussians, then activations will often be very close to 0 or 1, and learning will proceed very slowly.

Is there some way we can choose better initializations for the weights and biases, so that we don't get this kind of saturation, and so avoid a learning slowdown? Suppose we have a neuron with nin input weights. Then we shall initialize those weights as Gaussian random variables with mean 0 and standard deviation 1/n<sup>1/2</sup><sub>in</sub>. That is, we'll squash the Gaussians down, making it less likely that our neuron will saturate. We'll continue to choose the bias as a Gaussian with mean 0 and standard deviation 1, for reasons I'll return to in a moment. With these choices, the weighted sum z=\sum<sub>j</sub>w<sub>j</sub>x<sub>j</sub>+b will again be a Gaussian random variable with mean 0, but it'll be much more sharply peaked than it was before. Suppose, as we did earlier, that 500 of the inputs are zero and 500 are 1. Then it's easy to show (see the exercise below) that z has a Gaussian distribution with mean 0 and standard deviation (3/2)<sup>1/2</sup> = 1.22. This is much more sharply peaked than before, so much so that even the graph below understates the situation, since I've had to rescale the vertical axis, when compared to the earlier graph:

<p align="center">
  <img src="https://user-images.githubusercontent.com/57269172/157576160-f6d839c7-cba8-44b5-bb54-ec4a12aebdac.png" width="400"/><br>
  <b><i><a name="2.2"> Figure 1.2:</a></b> Z distribution with mean 0.</i>
</p>

Such a neuron is much less likely to saturate, and correspondingly much less likely to have problems with a learning slowdown.
